Logging to ./train_log
creating model and diffusion...
UNetModel(
  (time_embed): Sequential(
    (0): Linear(in_features=128, out_features=512, bias=True)
    (1): SiLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (input_blocks): ModuleList(
    (0): TimestepEmbedSequential(
      (0): Conv1d(5, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    )
    (1-3): 3 x TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Identity()
      )
    )
    (4): TimestepEmbedSequential(
      (0): Downsample(
        (op): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))
      )
    )
    (5): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=512, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
      )
    )
    (6-7): 2 x TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=512, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Identity()
      )
    )
    (8): TimestepEmbedSequential(
      (0): Downsample(
        (op): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))
      )
    )
    (9): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(256, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=768, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(256, 384, kernel_size=(1,), stride=(1,))
      )
      (1): AttentionBlock(
        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)
        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))
      )
    )
    (10-11): 2 x TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=768, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Identity()
      )
      (1): AttentionBlock(
        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)
        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))
      )
    )
    (12): TimestepEmbedSequential(
      (0): Downsample(
        (op): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))
      )
    )
    (13): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(384, 512, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=1024, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(384, 512, kernel_size=(1,), stride=(1,))
      )
      (1): AttentionBlock(
        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)
        (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      )
    )
    (14-15): 2 x TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=1024, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Identity()
      )
      (1): AttentionBlock(
        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)
        (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (middle_block): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=512, out_features=1024, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.0, inplace=False)
        (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      )
      (skip_connection): Identity()
    )
    (1): AttentionBlock(
      (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)
      (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))
      (attention): QKVAttentionLegacy()
      (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    )
    (2): ResBlock(
      (in_layers): Sequential(
        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=512, out_features=1024, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.0, inplace=False)
        (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      )
      (skip_connection): Identity()
    )
  )
  (output_blocks): ModuleList(
    (0-2): 3 x TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=1024, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
      )
      (1): AttentionBlock(
        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)
        (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      )
    )
    (3): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 896, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(896, 512, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=1024, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(896, 512, kernel_size=(1,), stride=(1,))
      )
      (1): AttentionBlock(
        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)
        (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      )
      (2): Upsample(
        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      )
    )
    (4): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 896, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(896, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=768, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(896, 384, kernel_size=(1,), stride=(1,))
      )
      (1): AttentionBlock(
        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)
        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))
      )
    )
    (5-6): 2 x TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(768, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=768, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(768, 384, kernel_size=(1,), stride=(1,))
      )
      (1): AttentionBlock(
        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)
        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))
      )
    )
    (7): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(640, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=768, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(640, 384, kernel_size=(1,), stride=(1,))
      )
      (1): AttentionBlock(
        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)
        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))
      )
      (2): Upsample(
        (conv): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))
      )
    )
    (8): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(640, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=512, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(640, 256, kernel_size=(1,), stride=(1,))
      )
    )
    (9-10): 2 x TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=512, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
    )
    (11): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=512, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(384, 256, kernel_size=(1,), stride=(1,))
      )
      (1): Upsample(
        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      )
    )
    (12): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(384, 128, kernel_size=(1,), stride=(1,))
      )
    )
    (13-15): 3 x TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (skip_connection): Conv1d(256, 128, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (out): Sequential(
    (0): GroupNorm32(32, 128, eps=1e-05, affine=True)
    (1): SiLU()
    (2): Conv1d(128, 5, kernel_size=(3,), stride=(1,), padding=(1,))
  )
)
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
¢u¢wSequential: 1-1                                  --
|    ¢|¢wLinear: 2-1                                 66,048
|    ¢|¢wSiLU: 2-2                                   --
|    ¢|¢wLinear: 2-3                                 262,656
¢u¢wModuleList: 1-2                                  --
|    ¢|¢wTimestepEmbedSequential: 2-4                --
|    |    ¢|¢wConv1d: 3-1                            2,048
|    ¢|¢wTimestepEmbedSequential: 2-5                --
|    |    ¢|¢wResBlock: 3-2                          230,400
|    ¢|¢wTimestepEmbedSequential: 2-6                --
|    |    ¢|¢wResBlock: 3-3                          230,400
|    ¢|¢wTimestepEmbedSequential: 2-7                --
|    |    ¢|¢wResBlock: 3-4                          230,400
|    ¢|¢wTimestepEmbedSequential: 2-8                --
|    |    ¢|¢wDownsample: 3-5                        49,280
|    ¢|¢wTimestepEmbedSequential: 2-9                --
|    |    ¢|¢wResBlock: 3-6                          591,872
|    ¢|¢wTimestepEmbedSequential: 2-10               --
|    |    ¢|¢wResBlock: 3-7                          657,408
|    ¢|¢wTimestepEmbedSequential: 2-11               --
|    |    ¢|¢wResBlock: 3-8                          657,408
|    ¢|¢wTimestepEmbedSequential: 2-12               --
|    |    ¢|¢wDownsample: 3-9                        196,864
|    ¢|¢wTimestepEmbedSequential: 2-13               --
|    |    ¢|¢wResBlock: 3-10                         1,232,000
|    |    ¢|¢wAttentionBlock: 3-11                   592,128
|    ¢|¢wTimestepEmbedSequential: 2-14               --
|    |    ¢|¢wResBlock: 3-12                         1,281,024
|    |    ¢|¢wAttentionBlock: 3-13                   592,128
|    ¢|¢wTimestepEmbedSequential: 2-15               --
|    |    ¢|¢wResBlock: 3-14                         1,281,024
|    |    ¢|¢wAttentionBlock: 3-15                   592,128
|    ¢|¢wTimestepEmbedSequential: 2-16               --
|    |    ¢|¢wDownsample: 3-16                       442,752
|    ¢|¢wTimestepEmbedSequential: 2-17               --
|    |    ¢|¢wResBlock: 3-17                         2,101,504
|    |    ¢|¢wAttentionBlock: 3-18                   1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-18               --
|    |    ¢|¢wResBlock: 3-19                         2,101,248
|    |    ¢|¢wAttentionBlock: 3-20                   1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-19               --
|    |    ¢|¢wResBlock: 3-21                         2,101,248
|    |    ¢|¢wAttentionBlock: 3-22                   1,051,648
¢u¢wTimestepEmbedSequential: 1-3                     --
|    ¢|¢wResBlock: 2-20                              --
|    |    ¢|¢wSequential: 3-23                       787,968
|    |    ¢|¢wIdentity: 3-24                         --
|    |    ¢|¢wSequential: 3-25                       525,312
|    |    ¢|¢wSequential: 3-26                       787,968
|    |    ¢|¢wIdentity: 3-27                         --
|    ¢|¢wAttentionBlock: 2-21                        --
|    |    ¢|¢wGroupNorm32: 3-28                      1,024
|    |    ¢|¢wConv1d: 3-29                           787,968
|    |    ¢|¢wQKVAttentionLegacy: 3-30               --
|    |    ¢|¢wConv1d: 3-31                           262,656
|    ¢|¢wResBlock: 2-22                              --
|    |    ¢|¢wSequential: 3-32                       787,968
|    |    ¢|¢wIdentity: 3-33                         --
|    |    ¢|¢wSequential: 3-34                       525,312
|    |    ¢|¢wSequential: 3-35                       787,968
|    |    ¢|¢wIdentity: 3-36                         --
¢u¢wModuleList: 1-4                                  --
|    ¢|¢wTimestepEmbedSequential: 2-23               --
|    |    ¢|¢wResBlock: 3-37                         3,413,504
|    |    ¢|¢wAttentionBlock: 3-38                   1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-24               --
|    |    ¢|¢wResBlock: 3-39                         3,413,504
|    |    ¢|¢wAttentionBlock: 3-40                   1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-25               --
|    |    ¢|¢wResBlock: 3-41                         3,413,504
|    |    ¢|¢wAttentionBlock: 3-42                   1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-26               --
|    |    ¢|¢wResBlock: 3-43                         3,151,104
|    |    ¢|¢wAttentionBlock: 3-44                   1,051,648
|    |    ¢|¢wUpsample: 3-45                         786,944
|    ¢|¢wTimestepEmbedSequential: 2-27               --
|    |    ¢|¢wResBlock: 3-46                         2,216,320
|    |    ¢|¢wAttentionBlock: 3-47                   592,128
|    ¢|¢wTimestepEmbedSequential: 2-28               --
|    |    ¢|¢wResBlock: 3-48                         2,019,456
|    |    ¢|¢wAttentionBlock: 3-49                   592,128
|    ¢|¢wTimestepEmbedSequential: 2-29               --
|    |    ¢|¢wResBlock: 3-50                         2,019,456
|    |    ¢|¢wAttentionBlock: 3-51                   592,128
|    ¢|¢wTimestepEmbedSequential: 2-30               --
|    |    ¢|¢wResBlock: 3-52                         1,822,592
|    |    ¢|¢wAttentionBlock: 3-53                   592,128
|    |    ¢|¢wUpsample: 3-54                         442,752
|    ¢|¢wTimestepEmbedSequential: 2-31               --
|    |    ¢|¢wResBlock: 3-55                         1,117,184
|    ¢|¢wTimestepEmbedSequential: 2-32               --
|    |    ¢|¢wResBlock: 3-56                         985,856
|    ¢|¢wTimestepEmbedSequential: 2-33               --
|    |    ¢|¢wResBlock: 3-57                         985,856
|    ¢|¢wTimestepEmbedSequential: 2-34               --
|    |    ¢|¢wResBlock: 3-58                         854,528
|    |    ¢|¢wUpsample: 3-59                         196,864
|    ¢|¢wTimestepEmbedSequential: 2-35               --
|    |    ¢|¢wResBlock: 3-60                         378,496
|    ¢|¢wTimestepEmbedSequential: 2-36               --
|    |    ¢|¢wResBlock: 3-61                         312,704
|    ¢|¢wTimestepEmbedSequential: 2-37               --
|    |    ¢|¢wResBlock: 3-62                         312,704
|    ¢|¢wTimestepEmbedSequential: 2-38               --
|    |    ¢|¢wResBlock: 3-63                         312,704
¢u¢wSequential: 1-5                                  --
|    ¢|¢wGroupNorm32: 2-39                           256
|    ¢|¢wSiLU: 2-40                                  --
|    ¢|¢wConv1d: 2-41                                1,925
===========================================================================
Total params: 58,634,373
Trainable params: 58,634,373
Non-trainable params: 0
===========================================================================
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
¢u¢wSequential: 1-1                                  [-1, 512]                 --
|    ¢|¢wLinear: 2-1                                 [-1, 512]                 66,048
|    ¢|¢wSiLU: 2-2                                   [-1, 512]                 --
|    ¢|¢wLinear: 2-3                                 [-1, 512]                 262,656
¢u¢wModuleList: 1                                    []                        --
|    ¢|¢wTimestepEmbedSequential: 2-4                [-1, 128, 64]             --
|    |    ¢|¢wConv1d: 3-1                            [-1, 128, 64]             2,048
|    ¢|¢wTimestepEmbedSequential: 2-5                [-1, 128, 64]             --
|    |    ¢|¢wResBlock: 3-2                          [-1, 128, 64]             230,400
|    ¢|¢wTimestepEmbedSequential: 2-6                [-1, 128, 64]             --
|    |    ¢|¢wResBlock: 3-3                          [-1, 128, 64]             230,400
|    ¢|¢wTimestepEmbedSequential: 2-7                [-1, 128, 64]             --
|    |    ¢|¢wResBlock: 3-4                          [-1, 128, 64]             230,400
|    ¢|¢wTimestepEmbedSequential: 2-8                [-1, 128, 32]             --
|    |    ¢|¢wDownsample: 3-5                        [-1, 128, 32]             49,280
|    ¢|¢wTimestepEmbedSequential: 2-9                [-1, 256, 32]             --
|    |    ¢|¢wResBlock: 3-6                          [-1, 256, 32]             591,872
|    ¢|¢wTimestepEmbedSequential: 2-10               [-1, 256, 32]             --
|    |    ¢|¢wResBlock: 3-7                          [-1, 256, 32]             657,408
|    ¢|¢wTimestepEmbedSequential: 2-11               [-1, 256, 32]             --
|    |    ¢|¢wResBlock: 3-8                          [-1, 256, 32]             657,408
|    ¢|¢wTimestepEmbedSequential: 2-12               [-1, 256, 16]             --
|    |    ¢|¢wDownsample: 3-9                        [-1, 256, 16]             196,864
|    ¢|¢wTimestepEmbedSequential: 2-13               [-1, 384, 16]             --
|    |    ¢|¢wResBlock: 3-10                         [-1, 384, 16]             1,232,000
|    |    ¢|¢wAttentionBlock: 3-11                   [-1, 384, 16]             592,128
|    ¢|¢wTimestepEmbedSequential: 2-14               [-1, 384, 16]             --
|    |    ¢|¢wResBlock: 3-12                         [-1, 384, 16]             1,281,024
|    |    ¢|¢wAttentionBlock: 3-13                   [-1, 384, 16]             592,128
|    ¢|¢wTimestepEmbedSequential: 2-15               [-1, 384, 16]             --
|    |    ¢|¢wResBlock: 3-14                         [-1, 384, 16]             1,281,024
|    |    ¢|¢wAttentionBlock: 3-15                   [-1, 384, 16]             592,128
|    ¢|¢wTimestepEmbedSequential: 2-16               [-1, 384, 8]              --
|    |    ¢|¢wDownsample: 3-16                       [-1, 384, 8]              442,752
|    ¢|¢wTimestepEmbedSequential: 2-17               [-1, 512, 8]              --
|    |    ¢|¢wResBlock: 3-17                         [-1, 512, 8]              2,101,504
|    |    ¢|¢wAttentionBlock: 3-18                   [-1, 512, 8]              1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-18               [-1, 512, 8]              --
|    |    ¢|¢wResBlock: 3-19                         [-1, 512, 8]              2,101,248
|    |    ¢|¢wAttentionBlock: 3-20                   [-1, 512, 8]              1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-19               [-1, 512, 8]              --
|    |    ¢|¢wResBlock: 3-21                         [-1, 512, 8]              2,101,248
|    |    ¢|¢wAttentionBlock: 3-22                   [-1, 512, 8]              1,051,648
¢u¢wTimestepEmbedSequential: 1-2                     [-1, 512, 8]              --
|    ¢|¢wResBlock: 2-20                              [-1, 512, 8]              --
|    |    ¢|¢wSequential: 3-23                       [-1, 512, 8]              787,968
|    |    ¢|¢wSequential: 3-24                       [-1, 1024]                525,312
|    |    ¢|¢wIdentity: 3-25                         [-1, 512, 8]              --
|    ¢|¢wAttentionBlock: 2-21                        [-1, 512, 8]              --
|    |    ¢|¢wGroupNorm32: 3-26                      [-1, 512, 8]              1,024
|    |    ¢|¢wConv1d: 3-27                           [-1, 1536, 8]             787,968
|    |    ¢|¢wQKVAttentionLegacy: 3-28               [-1, 512, 8]              --
|    |    ¢|¢wConv1d: 3-29                           [-1, 512, 8]              262,656
|    ¢|¢wResBlock: 2-22                              [-1, 512, 8]              --
|    |    ¢|¢wSequential: 3-30                       [-1, 512, 8]              787,968
|    |    ¢|¢wSequential: 3-31                       [-1, 1024]                525,312
|    |    ¢|¢wIdentity: 3-32                         [-1, 512, 8]              --
¢u¢wModuleList: 1                                    []                        --
|    ¢|¢wTimestepEmbedSequential: 2-23               [-1, 512, 8]              --
|    |    ¢|¢wResBlock: 3-33                         [-1, 512, 8]              3,413,504
|    |    ¢|¢wAttentionBlock: 3-34                   [-1, 512, 8]              1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-24               [-1, 512, 8]              --
|    |    ¢|¢wResBlock: 3-35                         [-1, 512, 8]              3,413,504
|    |    ¢|¢wAttentionBlock: 3-36                   [-1, 512, 8]              1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-25               [-1, 512, 8]              --
|    |    ¢|¢wResBlock: 3-37                         [-1, 512, 8]              3,413,504
|    |    ¢|¢wAttentionBlock: 3-38                   [-1, 512, 8]              1,051,648
|    ¢|¢wTimestepEmbedSequential: 2-26               [-1, 512, 16]             --
|    |    ¢|¢wResBlock: 3-39                         [-1, 512, 8]              3,151,104
|    |    ¢|¢wAttentionBlock: 3-40                   [-1, 512, 8]              1,051,648
|    |    ¢|¢wUpsample: 3-41                         [-1, 512, 16]             786,944
|    ¢|¢wTimestepEmbedSequential: 2-27               [-1, 384, 16]             --
|    |    ¢|¢wResBlock: 3-42                         [-1, 384, 16]             2,216,320
|    |    ¢|¢wAttentionBlock: 3-43                   [-1, 384, 16]             592,128
|    ¢|¢wTimestepEmbedSequential: 2-28               [-1, 384, 16]             --
|    |    ¢|¢wResBlock: 3-44                         [-1, 384, 16]             2,019,456
|    |    ¢|¢wAttentionBlock: 3-45                   [-1, 384, 16]             592,128
|    ¢|¢wTimestepEmbedSequential: 2-29               [-1, 384, 16]             --
|    |    ¢|¢wResBlock: 3-46                         [-1, 384, 16]             2,019,456
|    |    ¢|¢wAttentionBlock: 3-47                   [-1, 384, 16]             592,128
|    ¢|¢wTimestepEmbedSequential: 2-30               [-1, 384, 32]             --
|    |    ¢|¢wResBlock: 3-48                         [-1, 384, 16]             1,822,592
|    |    ¢|¢wAttentionBlock: 3-49                   [-1, 384, 16]             592,128
|    |    ¢|¢wUpsample: 3-50                         [-1, 384, 32]             442,752
|    ¢|¢wTimestepEmbedSequential: 2-31               [-1, 256, 32]             --
|    |    ¢|¢wResBlock: 3-51                         [-1, 256, 32]             1,117,184
|    ¢|¢wTimestepEmbedSequential: 2-32               [-1, 256, 32]             --
|    |    ¢|¢wResBlock: 3-52                         [-1, 256, 32]             985,856
|    ¢|¢wTimestepEmbedSequential: 2-33               [-1, 256, 32]             --
|    |    ¢|¢wResBlock: 3-53                         [-1, 256, 32]             985,856
|    ¢|¢wTimestepEmbedSequential: 2-34               [-1, 256, 64]             --
|    |    ¢|¢wResBlock: 3-54                         [-1, 256, 32]             854,528
|    |    ¢|¢wUpsample: 3-55                         [-1, 256, 64]             196,864
|    ¢|¢wTimestepEmbedSequential: 2-35               [-1, 128, 64]             --
|    |    ¢|¢wResBlock: 3-56                         [-1, 128, 64]             378,496
|    ¢|¢wTimestepEmbedSequential: 2-36               [-1, 128, 64]             --
|    |    ¢|¢wResBlock: 3-57                         [-1, 128, 64]             312,704
|    ¢|¢wTimestepEmbedSequential: 2-37               [-1, 128, 64]             --
|    |    ¢|¢wResBlock: 3-58                         [-1, 128, 64]             312,704
|    ¢|¢wTimestepEmbedSequential: 2-38               [-1, 128, 64]             --
|    |    ¢|¢wResBlock: 3-59                         [-1, 128, 64]             312,704
¢u¢wSequential: 1-3                                  [-1, 5, 64]               --
|    ¢|¢wGroupNorm32: 2-39                           [-1, 128, 64]             256
|    ¢|¢wSiLU: 2-40                                  [-1, 128, 64]             --
|    ¢|¢wConv1d: 2-41                                [-1, 5, 64]               1,925
====================================================================================================
Total params: 57,058,437
Trainable params: 57,058,437
Non-trainable params: 0
Total mult-adds (M): 417.24
====================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 4.61
Params size (MB): 217.66
Estimated Total Size (MB): 222.27
====================================================================================================
creating data loader...
training...
